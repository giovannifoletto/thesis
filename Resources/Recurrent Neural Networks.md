https://en.wikipedia.org/wiki/Recurrent_neural_network
=> the input of a layer can be its previously generated output


https://en.wikipedia.org/wiki/Long_short-term_memory
=> better support of RNN type model, with a "memory" that can persiste more steps and layers. 

https://en.wikipedia.org/wiki/Gated_recurrent_unit
=> GRU: it's like an RNN that can input or forget certain feature.

https://en.wikipedia.org/wiki/Generative_adversarial_network
=> used in generative AI, where two AI fight for some sort of improvement zero-sum (one lose when the other win).


RNN-LSTM
http://colah.github.io/posts/2015-08-Understanding-LSTMs/